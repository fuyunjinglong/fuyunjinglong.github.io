---
title: 0基础_数据结构
date: 2099-11-01 06:33:16
categories:
- E_数据结构
toc: true # 是否启用内容索引
---

# 算法学习

## **前言**

**第一阶段：简答书籍入手**

《大话数据结构》和《算法图解》，《趣学数据结构》和《趣学算法》

**第二阶段：简单视频加深印象**

[浙大陈越姥姥数据结构](https://www.bilibili.com/video/BV1JW411i731)

**第三阶段：实践学习-刷题**

- leetcode刷题，《剑指 offer》《编程珠玑》《编程之美》

- 左程云四大视频课(《算法和数据结构新手班》《算法和数据结构体系学习班》《大厂算法和数据结构刷题班》《每周有营养的大厂算法面试题》)

  算法和数据结构新手班：这个班唯一的目的就是为了让新手学员能熟悉一些算法上常见的coding、简单的算法。算是入门性质的预热课。
  算法和数据结构体系学习班：非常重要！再牛逼的学员也不可以跳过！讲述了大厂面试中必备的算法、数据结构、编程技巧、解题思路总结，还有188题的真实大厂面试题。
  大厂算法和数据结构刷题班：纯粹的题！但是不看完“算法和数据结构体系学习班”就来尝试，会非常痛苦。有350道题目。而且所有题目都和“算法和数据结构体系学习班”完全不重复！
  每周有营养的大厂算法面试题：目前每周三晚上8点直播，会讲一周以内出现的有意思的题，简称算法周更班。每周都是最新题。这个课是给报班的学员保持算法状态的班，但是最好是把上面的课弄完再来听

**第四阶段：经典书籍**

- 《算法》《算法导论》《计算机程序设计艺术》

## **大纲**

- [算法通关之路](https://github.com/azl397985856/leetcode)
- [吴师兄学算法](https://github.com/MisterBooo/LeetCodeAnimation)

## **参考-必备**

- [leetcode](https://leetcode.com/problemset/all/)
- [leetCode.cn](https://leetcode.cn/problemset/all/)
- [炼码](https://www.lintcode.com/)
- [九章算法](https://www.jiuzhang.com/seminar/?page=1)
- [UOJ](https://uoj.ac/problems)
- [牛客网](https://www.nowcoder.com/exam/company)
- [afatcoder整理题库](https://github.com/afatcoder/LeetcodeTop)

**参考-竞赛**

- [北大POJ](http://poj.org/)
- [杭电HDU](http://acm.hdu.edu.cn/)

## 如何刷题

- [大家都是如何刷 LeetCode 的？](https://www.zhihu.com/question/280279208)
- [刷完 LeetCode 是什么水平？能拿到什么水平的 offer？](https://www.zhihu.com/question/32019460)
- [LeetCode按照怎样的顺序来刷题比较好？](https://www.zhihu.com/question/36738189)

**leetcode闯关**

 **第一轮**

题目类别推荐按照这个顺序来说： **数组 ->字符串 ->链表->二分查找->排序->哈希表-> 栈->队列 ->树 、递归、回溯 -> 堆** ，每一个类别只做简单的题目！而且是从通过率又高向低来刷。

**第二轮**

开始做第一轮类别里面中等难度的题目

**第三轮**

这一轮的题目所代表的算法是比较难的，如果之前没有了解过，直接就刷题会很难，所以，建议把相关算法的理论看一遍，再去刷题， **先了解一下 什么是 拓扑排序、 分治，搜索树，贪心，动态规划，以及深搜和广搜**，然后再去刷题

推荐按照这个顺序去做：**拓扑排序->分治算法-> 二叉搜索树->贪心算法->动态规划-> 深度优先搜索 -> 广度优先搜索-->图->数学**

这里依然还是只做简单难度的题目 按照通过率又高向低

**第四轮**

开始尝试做第一轮里面的 困难难度的题目 （不强求，仅仅是尝试，面试中也不会太难）

同时开始做第二轮里面 中等难度的题目 （不用全都做，选一下自己一看就有想法的题目）

**第五轮**

然后开始做做leetcode分类里面 第一轮和第二轮都没有涉及到的类别的题目，相对于其他类型，面试中最常考察的是： **字典树-> 并查集 -> 树状数组->线段树 ， 这里只做简单难度的就可以了！**

剩余类型题目也凭个人喜好，选一些简单难度的题目做一做就可以了。

**总结**

达到面试的水平**一般到第三轮就可以了**，至于要刷多少题呢，

不得不说，现在leetcode上的题太多了，我当时刷leetcode的时候 才500题左右，我是刷了300道才开始面试的，现在的话，**我建议刷题刷到400到以上为佳**。 而且要注意每轮里面的各个类型都要做到！

# 1.面向对象

(1)面向过程与面向对象

面向过程：早期我们使用的是过程变成，即自己是执行者，执行代码。

面向对象：自己则变成了指挥者，从始至终都是自己去命令、调度其他资源完成任务。更加符合人思考习惯，从执行者变成指挥者。

(2)面向对象4大特性

软件编程就是将我们的思想转变成计算机能够识别的语言的过程。

面向对象是把一组数据结构和处理他们的方法组成对象。

抽象、封装、继承、多态

- 抽象：把具有相同行为的对象归为类
- 封装：通过封装隐藏类的内部细节
- 继承：通过继承使类得到泛化
- 多态：通过多态实现基于对象类型的动态分配

**面向对象四大特性**

抽象：**提高代码的扩展性、可维护性，降低复杂度**。对方法的具体实现进行隐藏，让调用者只需要关心方法

封装：**保护数据和隐藏信息**，访问权限控制。通过暴露有限的方法来达到保护数据的作用

继承：**解决代码复用问题，提升可维护性**。继承最大的作用就是用来实现更好的代码复用，减少重复代码。

多态：**提高代码的可扩展性和复用性**。需要编程语言提供特殊的语法机制，通过多态使代码变得更加灵活。“同一个事件发生在不同的对象上会产生不同的结果”

# 2.为什么大厂螺丝钉的岗位需要算法很牛逼人

算法是绝对代码能力和耐心的一种证明。算法是智力程度的良好证明。老的框架技术已经没办法筛查人才。

**算法是绝对代码能力和耐心的一种证明**

好公司考算法与数据结构的比重是比较大的，以前大公司考的现在比重越来越高，现在小公司以前不考的现在也开始考了。早期的程序员是非常在乎算法数据机构，比如阿波罗登月，飞船的控制程序内存不足1M，当时那批老派程序员以极强的算法与数据结构把有限的资源榨取到极致，充分发挥自己在算法与数据结构的设计和能力。在一个很吃紧的资源下也能完成很复杂的任务。

大公司的面经经过记忆性的知识，是可以基本应付的，但是算法不行。你需要理解基础算法与数据结构，还要触类旁通，举一反三，逻辑推导。所以我们国内开始考察这些方向，同时美国外国公司早就考过算法了。因为你进入到工作岗位，记忆性的知识是可以查文档的，你是可以现场现学的，哪怕是面向google编程。但是算法不行，因为门槛比较高。

国外公司只考算法与数据结构，是因为杜绝了其他方向是重要的，或者说对于一个即将进入职场，每天可以查询网络资料的人，记忆性的东西没那么重要。有一个段子，若干年之前，google招了一个接线员，宾夕法尼亚的法律系博士，这行业内卷到这个程度？google招聘精英基本是传统，一个电话接线员都招这么贵，为什么？google招聘两个员工，A员工比B员工的代码性能提升5%-7%，到代码部署到用户真实使用的场景下，带来的收益远远超过招聘成本本身。所以坊间流传着一句话：最贵的就是最省的。可能你觉得美国计算机专业前三的名校的博士，每年给那呢么多钱，然后也写很简单的业务，你觉得是浪费？但是不是的。

招精英员工即最贵的就是最省的。代码性能提升5%到10%，可以省下昂贵的开销。
以前是糙快猛，现在公司板块领域瓜分完了，就进入仔细耕耘，好好迭代，优化代码，提升性能。

**算法是智力程度的良好证明**

刷题，刷题，刷题。从隐隐理解算法，到真正写出算法，不是一般人能坚持的。如果对算法或代码面试稍微了解的人，都知道，公司实际出题目可能是在白纸或编辑器上写出代码的，有些面试官可能因为你一点点边界条件没考虑好，就直接扣分或让你走人都有可能。

**老的框架技术已经没办法筛查人才**

技术知识可以通过记忆性的东西还原，但是算法不行，必须需要理解并掌握核心思想。

# 3.经典排序算法

**算法分类**

- 内部排序(交叉选轨迹)

  交换：冒泡、快排

  插入：直接插入、希尔排序

  选择:简单选择、堆排序

  归并排序

  基数排序

- 外部排序

![image-20220113223656269](/img/image-20220113223656269.png)

# 4.topK问题

**100亿数据找出最大的1000个数字（top K问题）**

***对于海量数据处理，思路基本上是：必须分块处理，然后再合并起来。***

**a.理论方法：**

1. 全部排序
2. 局部淘汰法
3. 分治法
4. Hash法

**1.全部排序**

**最容易想到的方法是将数据全部排序**。该方法并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。

**2.局部淘汰法**

用一个容器保存前10000个数，然后将剩余的所有数字一一与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小。

这个容器可以用（小顶堆）最小堆来实现。我们知道完全二叉树有几个非常重要的特性，就是假如该二叉树中总共有N个节点，那么该二叉树的深度就是log2N，对于小顶堆来说移动根元素到 底部或者移动底部元素到根部只需要log2N，相比N来说时间复杂度优化太多了（1亿的logN值是26-27的一个浮点数）。基本的思路就是先从文件中取出1000个元素构建一个小顶堆数组k，然后依次对剩下的100亿-1000个数字进行遍历m，如果m大于小顶堆的根元素，即k[0]，那么用m取代k[0]，对新的数组进行重新构建组成一个新的小顶堆。这个算法的时间复杂度是O((100亿-1000)log(1000))，即O((N-M)logM)，空间复杂度是M

这个算法优点是性能尚可，空间复杂度低，IO读取比较频繁，对系统压力大。

**3.分治法**

a、将100亿个数据分为1000个大分区，每个区1000万个数据

b、每个大分区再细分成100个小分区。总共就有1000*100=10万个分区

c、计算每个小分区上最大的1000个数。

> 为什么要找出每个分区上最大的1000个数？举个例子说明，全校高一有100个班，我想找出全校前10名的同学，很傻的办法就是，把高一100个班的同学成绩都取出来，作比较，这个比较数据量太大了。应该很容易想到，班里的第11名，不可能是全校的前10名。也就是说，不是班里的前10名，就不可能是全校的前10名。因此，只需要把每个班里的前10取出来，作比较就行了，这样比较的数据量就大大地减少了。我们要找的是100亿中的最大1000个数，所以每个分区中的第1001个数一定不可能是所有数据中的前1000个。

d、合并每个大分区细分出来的小分区。每个大分区有100个小分区，我们已经找出了每个小分区的前1000个数。将这100个分区的1000*100个数合并，找出每个大分区的前1000个数。

e、合并大分区。我们有1000个大分区，上一步已找出每个大分区的前1000个数。我们将这1000*1000个数合并，找出前1000.这1000个数就是所有数据中最大的1000个数。

（a、b、c为map阶段，d、e为reduce阶段）

**4.Hash法**

如果这1亿个书里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。

**b.实战解法**

1.立刻就能想到的解法:快排

2.O(n) 时间复杂度的方法：快排的 partition 划分思想可以用于计算某个位置的数值等问题。虽然时间复杂度是 O(n) ，但是缺点也很明显，最主要的就是内存问题，在海量数据的情况下，我们很有可能没办法一次性将数据全部加载入内存，这个时候这个方法就无法完成使命了

3.利用分布式思想处理海量数据

4.利用最经典的方法，一台机器也能处理海量数据

[leecode40](https://leetcode.cn/problems/zui-xiao-de-kge-shu-lcof/solution/tu-jie-top-k-wen-ti-de-liang-chong-jie-fa-you-lie-/)

其实提到 Top K 问题，最经典的解法还是利用堆。

维护一个大小为 K 的小顶堆，依次将数据放入堆中，当堆的大小满了的时候，只需要将堆顶元素与下一个数比较：如果大于堆顶元素，则将当前的堆顶元素抛弃，并将该元素插入堆中。遍历完全部数据，Top K 的元素也自然都在堆里面了。

当然，如果是求前 K 个最小的数，只需要改为大顶堆即可



**MapReduce**

[一文读懂MapReduce原文](https://juejin.cn/post/6844903812784717831)

MapReduce 是一个高性能的分布式计算框架，用于大规模数据集（大于1TB）的并行运算。它极大的方便编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。MapReduce分为Map(映射)和Reduce(化简)两个阶段，是在HDFS存储数据的基础上，将一个较大的计算任务(job)分解成若干小任务(task)，每个小任务都由一个Map任务(task)来计算（这个Map尽量在数据所在节点上完成计算），然后再将每个Map的计算结果由一个或多个Reduce任务(task)合并，得到最终的结果。

# 5.常见算法-动态规划算法

动态规划是算法设计中的一种方法，一种算法设计思想。它将一个问题分解为相互重叠的子问题，通过反复求解子问题，来解决原来的问题。

## **举例算法题：爬楼梯**

假设你正在爬楼梯。需要 n 阶你才能到达楼顶。
每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？
注意：给定 n 是一个正整数。

**示例 1：**

```html
输入： 2
输出： 2
解释： 有两种方法可以爬到楼顶。
1.  1 阶 + 1 阶
2.  2 阶
```

**示例 2：**

```html
输入： 3
输出： 3
解释： 有三种方法可以爬到楼顶。
1.  1 阶 + 1 阶 + 1 阶
2.  1 阶 + 2 阶
3.  2 阶 + 1 阶
```

**解题思路：**

- 爬到第n阶可以在第n-1阶爬1个台阶，或者在第n-2阶爬2个台阶.
- F(n)=F(n-1)+F(n-2)。
- 使用动态规划。

**解题步骤：**

- 定义子问题：F(n)=F(n-1)+F(n-2)。
- 反复执行：从2循环到n,执行上述公式。

**解体答案：**

**1、时间复杂度O(n)，空间复杂度O(n)**

```js
var climbStairs = function(n){
    //边界条件
    if(n < 2){ return 1; }
    const dp = [1, 1]; //记录第n阶有多少种方法
    for(let i = 2; i<=n; i=+1){
        dp[i] = dp[i-1]+dp[i-2];
    }
    console.log(dp);
    return dp[n];
}
```

**2、时间复杂度O(n) 空间复杂度O(1)**

```js
var climbStairs = function(n){
    //边界条件
    if(n < 2){ return 1; }
    let dp0 = 1;
    let dp1 = 1
    for(let i = 2; i<=n; i=+1){
        const temp = dp0;
        dp0 = dp1;
        dp1 = dp1 + temp;
    }
    return dp1;
}
```

# 常见算法-回溯算法

回溯算法是算法设计中的一种方法，算法的一种设计思想。是一种渐进式寻找并构建问题解决方式的策略。回溯算法会从一个可能的动作开始解决问题，如果不行，就回溯并选择另一个动作，直到将问题解决。

**什么问题适合用回溯算法解决：**
有很多路。
这些路里，有死路，也有出路。
通常需要递归来模拟所有的路。

**例如：全排列问题：**
用递归模拟出所有情况。
遇到包含重复元素的情况，就回溯（不要在遍历）。
收集所有到达递归终点的情况，并返回。

## **举例算法题一：全排列**

给定一个 没有重复 数字的序列，返回其所有可能的全排列。
**示例:**
**输入:** [1,2,3]
**输出:**

```json
[
  [1,2,3],
  [1,3,2],
  [2,1,3],
  [2,3,1],
  [3,1,2],
  [3,2,1]
]
```

**解题思路：**

前提要求：1、所有排列情况；2、没有重复元素

- 有出路，有死路
- 考虑使用回溯算法

**解题步骤：**

1、用递归模拟出所有情况。
2、遇到包含重复元素的情况，就回溯（不在递归下去）。
3、收集所有到达终点的情况，并返回。

**解题答案：**

```js
var permute = function(nums){
    const res = []; //所有出路
    const backrack = (path) =>{ //递归
        //收集满足要求的排列组合,递归的终点
        if(path.length === nums.length){
            res.push(path);
            return;
        }
        nums.forEach(n => {
            if(path.includes(n)){ return;} //一些路是死路,就回溯
            backrack(path.concat(n));  //下一次递归加到数组里面
        });
    } 
    backrack([]); //[1,2,3]
    return res;
}

var arr =  [1,2,3,4]
console.log(permute(arr))
```

算法分析：

时间复杂度最复杂的，嵌套的for循环，是 O(n!),n!= 1*2*3#…(n-1)*n; 
空间复杂度是，输出的数组用到递归（堆栈），递归的深度N：O(n)。

## **举例算法题二：子集**

给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。
解集 不能 包含重复的子集。你可以按 任意顺序 返回解集。

**示例 1：**

```html
输入：nums = [1,2,3]
输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]]
```

**示例 2：**

```html
输入：nums = [0]
输出：[[],[0]]
```

**提示：**

- `1 <= nums.length <= 10`
- `-10 <= nums[i] <= 10`
- `nums` 中的所有元素 **互不相同**

解题思路：

要求：1、所有的子集；2、没有重复元素和子集
有出路，有死路
考虑使用回溯算法

**解题步骤：**

1、用递归模拟出所有情况。写递归函数遍历每一个数。
2、保证接的数字都是后面的数字。
3、收集所有到达递归终点的情况，并返回。

解题答案：

```js
var subsets = function(nums){
    const res = []; //用来最为最终输出的大数组
    const backtrack = (path, l, start) =>{ //backtrack回溯的意思,参数是路径、长度、下标
        if(path.length === l){ //收集到终点的情况
            res.push(path);
            return;
        }
        for(let i = start; i <= nums.length; i += 1){
            backtrack(path.concat(nums[i]), l, i + 1);
        }
    }
    for(let i = 0; i <= nums.length; i += 1){
        backtrack([],i,0) //参数是路径、长度、下标
    }
    return res;
}

var arr =  [1,2,3]
console.log(subsets(arr))
```

**算法分析：**

时间复杂度:O(2^N),因为每个元素都有可能有两种可能（存在或不存在）；
空间复杂度是O(N) 递归堆栈放的临时变量。

## 其他回溯法

[最全排列组合问题](https://blog.csdn.net/bitcarmanlee/article/details/114500993)

# 常见算法-分而治之算法

分而治之是算法设计中的一种方法，算法的一种设计思想。它将一个问题分成多个和原问题相似的小问题，递归解决小问题，再将结果合并一解决原来的问题。

**使用场景一： 归并排序**

分：把数组从中间一分为二。
解：递归地对两个子数组进行归并排序。
合：合并有序子数组。

**使用场景二： 快速排序**

分：选基准，按基准把数组分成两个子数组。
解：递归地对两个子数组进行快速排序。
合：对两个子数组进行合并。

## **举例算法题一：猜数字大小**

**猜数字游戏的规则如下：**

每轮游戏，我都会从 1 到 n 随机选择一个数字。 请你猜选出的是哪个数字。
如果你猜错了，我会告诉你，你猜测的数字比我选出的数字是大了还是小了。
你可以通过调用一个预先定义好的接口 int guess(int num) 来获取猜测结果，返回值一共有 3 种可能的情况（-1，1 或 0）：

-1：我选出的数字比你猜的数字小 pick < num 1：我选出的数字比你猜的数字大 pick > num
0：我选出的数字和你猜的数字一样。恭喜！你猜对了！pick == num
返回我选出的数字。

**示例 1：**
输入：n = 10, pick = 6
输出：6

**示例 2：**
输入：n = 1, pick = 1
输出：1

**示例 3：**
输入：n = 2, pick = 1
输出：1

**示例 4：**
输入：n = 2, pick = 2
输出：2

**提示：**
1 <= n <= 2^31 – 1
1 <= pick <= n

解题思路

分：计算中间元素，分割数组。
解:递归地在较大或者较小数组进行二分搜索。
合：不需要此步，因为在子数组中搜到就返回了。

**解题答案**

```js
var guessNumber = function(n){
    const rec = (low,high) =>{
        if(low > high){ return }
        const mid = Math.floor((low+high) / 2);
        const res = guess(mid);
        if(res === 0){
            return mid;
        }else if(res === 1){
            return rec(mid + 1,high);
        }else{
            return rec(1, mid - 1);
        }
    }
    return rec(1,n);
}
```

**解题分析**

时间复杂度 O(logn)，空间复杂度 O(logn)

## **举例算法题二：翻转二叉树**

翻转一棵二叉树。
**示例：**
**输入：**

```js
     4
   /   \
  2     7
 / \   / \
1   3 6   9
```

**输出：**

```js
     4
   /   \
  7     2
 / \   / \
9   6 3   1
```

解题思路：

- 先反转左右子树，再将子树换个位置。
- 符合“分、解、和”特性。
- 考虑用分而治之来做。

解题步骤：

分：获取左右子树。
解：递归地翻转左右子树。
合：将翻转后的左右子树换个位置放到根节点上。

解题答案：

```js
var invertTree = function(root){
    if(!root){ return null; }
    return {
        val: root.val,
        left: invertTree(root.right),
        right: invertTree(root.left)
    }
}
```

解题分析：

时间复杂度O(n)，空间复杂度 O(n)。

## **举例算法题三：相同的树**

给你两棵二叉树的根节点 p 和 q ，编写一个函数来检验这两棵树是否相同。
如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。

**示例 1：**

![分而治之算法-贺鹏飞的博客](https://www.hepengfei.net/wp-content/uploads/2021/04/ex1.jpg)

```
输入：p = [1,2,3], q = [1,2,3]
输出：true
```

**示例 2：**

![分而治之算法-贺鹏飞的博客](https://www.hepengfei.net/wp-content/uploads/2021/04/ex2.jpg)

```
输入：p = [1,2], q = [1,null,2]
输出：false
```

**示例 3：**

![分而治之算法-贺鹏飞的博客](https://www.hepengfei.net/wp-content/uploads/2021/04/ex3.jpg)

```
输入：p = [1,2,1], q = [1,1,2]
输出：false
```

**提示：**

- 两棵树上的节点数目都在范围 `[0, 100]` 内
- `-104 <= Node.val <= 104`

解题思路：

- 两棵树：根节点的值相同，左子树相同，右子树相同
- 符合“分、解、和”特性。
- 考虑用分而治之来做。

**解题步骤：**

分：获取两个树的左子树和右子树。
解：递归地判断两个数的左子树是否相同，右子树是否相同。
合：将上述结果合并，如果根节点的值也相同，树就相同。

解题答案：

```js
var inSameTree = function(p,q){
    if(!p && !q){ return true} //如果两个都为空
    if( p && q && p.val === q.val && 
        inSameTree(p.left,q.left )&& 
        inSameTree(p.right,q.right)
     ){
         return true;
     }
     return false;
}
```

## **举例算法题四：对称二叉树**

给定一个二叉树，检查它是否是镜像对称的。
例如，二叉树 `[1,2,2,3,4,4,3]` 是对称的。

```js
    1
   / \
  2   2
 / \ / \
3  4 4  3
```

但是下面这个 `[1,2,2,null,3,null,3]` 则不是镜像对称的:

```js
    1
   / \
  2   2
   \   \
   3    3
```

解题思路：

- 转化为：左右子树是否镜像。
- 分解为：树1的左子树和树2的右子树是否镜像，树1的右子树和树2的左子树是否镜像。
- 符合“分、解、和”特性，考虑用分而治之来做。

解题步骤：

分：获取两个树的左子树和右子树。
解：递归地判断树1的左子树和树2的右子树是否镜像，树1的右子树和树2的左子树是否镜像。
合：如果上述都成立，且根节点值也相同，两个树就镜像。

解题答案：

```js
var isSymmetric = function(root){
    if(!root) return true;
    const isMirror = (l, r) => {
        if(!l && !r) return true;
        if(l && r && l.val === r.val &&
            isMirror(l.left, r.right)&&
            isMirror(l.right, r.left)
          ){
              return true;
          }
          return false;
    }
    return isMirror(root.left, root.right);
}
```

解题分析：

时间复杂度O(n)，空间复杂度 O(LogN)和O(n)。

# 常见算法-贪心算法

贪心算法是算法设计中的一种方法，算法的一种设计思想，期盼通过每个阶段的局部最优选择，从而达到全局的最优，结果并不一定是最优。

## **举例算法题一：分发饼干**

假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。

对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] >= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。

**示例 1:**

```plane
输入: g = [1,2,3], s = [1,1]
输出: 1
解释:
你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。
虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。
所以你应该输出1。
```

**示例 2:**

```plane
输入: g = [1,2], s = [1,2,3]
输出: 2
解释:
你有两个孩子和三块小饼干，2个孩子的胃口值分别是1,2。
你拥有的饼干数量和尺寸都足以让所有孩子满足。
所以你应该输出2.
```

**提示：**

- `1 <= g.length <= 3 * 104`
- `0 <= s.length <= 3 * 104`
- `1 <= g[i], s[j] <= 231 - 1`

解题思路：

- 局部最优：既能满足孩子，还消耗最少。
- 先将“较小的饼干”分给“胃口最小”的孩子。

解题步骤：

- 对饼干数组和胃口数组升序排序。
- 遍历饼干数组，找到能满足第一个孩子的饼干。
- 然后继续遍历饼干数组，找到满足第二、三、……、n个孩子的饼干。

解题答案：

```js
var findContentChildren = function(g,s){
    //升序排序
    const sortFunc = function(a,b){
        return a - b; //实现降序 b - a
    }
    g.sort(sortFunc);
    s.sort(sortFunc);
    let i = 0;
    s.forEach(n => {
        if(n >= g[i]){
            i += 1;
        }
    });
    return i;
}
```

## **举例算法题二：买卖股票的最佳时机 II**

给定一个数组 prices ，其中 prices[i] 是一支给定股票第 i 天的价格。
设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。
注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。

**示例 1:**

```plane
输入: prices = [7,1,5,3,6,4]
输出: 7
解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。
     随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。
```

**示例 2:**

```plane
输入: prices = [1,2,3,4,5]
输出: 4
解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。
     注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。
```

**示例 3:**

```plane
输入: prices = [7,6,4,3,1]
输出: 0
解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。
```

**提示：**

- `1 <= prices.length <= 3 * 104`
- `0 <= prices[i] <= 104`

解题思路：

- 前提：上帝视角，知道未来的价格。
- 局部最优：见好就收，见差就不动，不做任何长远打算。

解题步骤：

- 新建一个变量，用来统计总利润。
- 遍历价格数组，如果当前价格比昨天高，就在昨天买，今天卖，否组就不交易。
- 遍历结束后，返回所有利润之和。

解题答案：

```js
var maxProfit = function(prices){
    let profit = 0; //利润
    for(let i = 1; i < prices.length; i += 1){
        if(prices[i] > prices[i - 1]){ //当前价格大于昨天价格
            profit += prices[i] - prices[i - 1]
        }
    }
    return profit;
}

var prices = [7,1,5,3,6,4]
console.log(maxProfit(prices)) //7

var prices = [1,2,3,4,5]
console.log(maxProfit(prices)) //4

var prices = [7,6,4,3,1]
console.log(maxProfit(prices)) //0
```

解题分析：

时间复杂度 有一个for循环 O(n) ，空间复杂度 O(1)。

# 6.LRU算法

LRU算法全称是最近最少使用算法（Least Recently Use），广泛的应用于缓存机制中。当缓存使用的空间达到上限后，就需要从已有的数据中淘汰一部分以维持缓存的可用性，而淘汰数据的选择就是通过LRU算法完成的。

LRU算法的基本思想是基于局部性原理的时间局部性：

> 如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。

所以顾名思义，LRU算法会选出最近最少使用的数据进行淘汰。

## 原理

一般来讲，LRU将访问数据的顺序或时间和数据本身维护在一个容器当中。当访问一个数据时：

1. 该数据不在容器当中，则设置该数据的优先级为最高并放入容器中。
2. 该数据在容器当中，则更新该数据的优先级至最高。

当数据的总量达到上限后，则移除容器中优先级最低的数据。下图是一个简单的LRU原理示意图：

<img src="/img/image-20220605095455588.png" alt="image-20220605095455588" style="zoom:67%;" />

如果我们按照`7 0 1 2 0 3 0 4`的顺序来访问数据，且数据的总量上限为3，则如上图所示，LRU算法会依次淘汰`7 1 2`这三个数据。

## 朴素的LRU算法

那么我们现在就按照上面的原理，实现一个朴素的LRU算法。下面有三种方案：

1. 基于数组

   方案：为每一个数据附加一个额外的属性——时间戳，当每一次访问数据时，更新该数据的时间戳至当前时间。当数据空间已满后，则扫描整个数组，淘汰时间戳最小的数据。

   不足：维护时间戳需要耗费额外的空间，淘汰数据时需要扫描整个数组。

2. 基于长度有限的双向链表

   方案：访问一个数据时，当数据不在链表中，则将数据插入至链表头部，如果在链表中，则将该数据移至链表头部。当数据空间已满后，则淘汰链表最末尾的数据。

   不足：插入数据或取数据时，需要扫描整个链表。

3. 基于双向链表和哈希表

   方案：为了改进上面需要扫描链表的缺陷，配合哈希表，将数据和链表中的节点形成映射，将插入操作和读取操作的时间复杂度从O(N)降至O(1)

### **基于双向链表 + 哈希表实现LRU**

下面我们就基于双向链表和哈希表实现一个LRU算法

```
public class LRUCache {
    private int size; // 当前容量
    private int capacity; // 限制大小
    private Map<Integer, DoubleQueueNode> map; // 数据和链表中节点的映射
    private DoubleQueueNode head; // 头结点 避免null检查
    private DoubleQueueNode tail; // 尾结点 避免null检查
    
    public LRUCache(int capacity) {
        this.capacity = capacity;
        this.map = new HashMap<>(capacity);
        this.head = new DoubleQueueNode(0, 0);
        this.tail = new DoubleQueueNode(0, 0);
        this.head.next = tail;
    }

    public Integer get(Integer key) {

        DoubleQueueNode node = map.get(key);
        if (node == null) {
            return null;
        }

        // 数据在链表中，则移至链表头部
        moveToHead(node);

        return node.val;
    }

    public Integer put(Integer key, Integer value) {

        Integer oldValue;
        DoubleQueueNode node = map.get(key);
        if (node == null) {
            // 淘汰数据
            eliminate();
            // 数据不在链表中，插入数据至头部
            DoubleQueueNode newNode = new DoubleQueueNode(key, value);
            DoubleQueueNode temp = head.next;
            head.next = newNode;
            newNode.next = temp;
            newNode.pre = head;
            temp.pre = newNode;
            map.put(key, newNode);
            size++;
            oldValue = null;
        } else {
            // 数据在链表中，则移至链表头部
            moveToHead(node);
            oldValue = node.val;
            node.val = value;
        }
        return oldValue;
    }

    public Integer remove(Integer key) {
        DoubleQueueNode deletedNode = map.get(key);
        if (deletedNode == null) {
            return null;
        }
        deletedNode.pre.next = deletedNode.next;
        deletedNode.next.pre = deletedNode.pre;
        map.remove(key);
        return deletedNode.val;
    }
    
    // 将节点插入至头部节点
    private void moveToHead(DoubleQueueNode node) {
        node.pre.next = node.next;
        node.next.pre = node.pre;
        DoubleQueueNode temp = head.next;
        head.next = node;
        node.next = temp;
        node.pre = head;
        temp.pre = node;
    }

    private void eliminate() {
        if (size < capacity) {
            return;
        }
        
        // 将链表中最后一个节点去除
        DoubleQueueNode last = tail.pre;
        map.remove(last.key);
        last.pre.next = tail;
        tail.pre = last.pre;
        size--;
        last = null;
    }
}

// 双向链表节点
class DoubleQueueNode {
    int key;
    int val;
    DoubleQueueNode pre;
    DoubleQueueNode next;
    public DoubleQueueNode(int key, int val) {
        this.key = key;
        this.val = val;
    }
}
```

基本上就是把上述LRU算法思路用代码实现了一遍，比较简单，只需要注意一下pre和next两个指针的指向和同步更新哈希表，put()和get()操作的时间复杂度都是O(1)，空间复杂度为O(N)。

### 基于LinkedHashMap实现的LRU

其实我们可以直接根据JDK给我们提供的LinkedHashMap直接实现LRU。因为LinkedHashMap的底层即为双向链表和哈希表的组合，所以可以直接拿来使用。

```
public class LRUCache extends LinkedHashMap {

    private int capacity;

    public LRUCache(int capacity) {
        // 注意这里将LinkedHashMap的accessOrder设为true
        super(16, 0.75f, true);
        this.capacity = capacity;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry eldest) {
        return super.size() >= capacity;
    }
}

复制代码
```

默认LinkedHashMap并不会淘汰数据，所以我们重写了它的removeEldestEntry()方法，当数据数量达到预设上限后，淘汰数据，accessOrder设为true意为按照访问的顺序排序。整个实现的代码量并不大，主要都是应用LinkedHashMap的特性。

正因为LinkedHashMap这么好用，所以我们可以看到Dubbo的LRU缓存[LRUCache](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fapache%2Fdubbo%2Fblob%2F7a48fac84b14ac6a21c1bdfc5958705dd8dda84d%2Fdubbo-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdubbo%2Fcommon%2Futils%2FLRUCache.java%23L23)也是基于它实现的。

## LRU算法优化

朴素的LRU算法已经能够满足缓存的要求了，但是还是有一些不足。当热点数据较多时，有较高的命中率，但是如果有偶发性的批量操作，会使得热点数据被非热点数据挤出容器，使得缓存受到了“污染”。所以为了消除这种影响，又衍生出了下面这些优化方法。

### LRU-K

LRU-K算法是对LRU算法的改进，将原先进入缓存队列的评判标准从访问一次改为访问K次，可以说朴素的LRU算法为LRU-1。

LRU-K算法有两个队列，一个是缓存队列，一个是数据访问历史队列。当访问一个数据时，首先先在访问历史队列中累加访问次数，当历史访问记录超过K次后，才将数据缓存至缓存队列，从而避免缓存队列被污染。同时访问历史队列中的数据可以按照LRU的规则进行淘汰。具体如下图所示：

<img src="/img/image-20220605095724949.png" alt="image-20220605095724949" style="zoom:67%;" />

下面我们来实现一个LRU-K缓存：

```
// 直接继承我们前面写好的LRUCache
public class LRUKCache extends LRUCache {
    
    private int k; // 进入缓存队列的评判标准
    private LRUCache historyList; // 访问数据历史记录

    public LRUKCache(int cacheSize, int historyCapacity, int k) {
        super(cacheSize);
        this.k = k;
        this.historyList = new LRUCache(historyCapacity);
    }

    @Override
    public Integer get(Integer key) {

        // 记录数据访问次数
        Integer historyCount = historyList.get(key);
        historyCount = historyCount == null ? 0 : historyCount;
        historyList.put(key, ++historyCount);

        return super.get(key);
    }

    @Override
    public Integer put(Integer key, Integer value) {

        if (value == null) {
            return null;
        }
        
        // 如果已经在缓存里则直接返回缓存中的数据
        if (super.get(key) != null) {
            return super.put(key, value);;
        }

        // 如果数据历史访问次数达到上限，则加入缓存
        Integer historyCount = historyList.get(key);
        historyCount = historyCount == null ? 0 : historyCount;
        if (historyCount >= k) {
            // 移除历史访问记录
            historyList.remove(key);
            return super.put(key, value);
        }
    }
}
```

一般来讲，当K的值越大，则缓存的命中率越高，但是也会使得缓存难以被淘汰。综合来说，使用LRU-2的性能最优。

### Two Queue

Two Queue可以说是LRU-2的一种变种，将数据访问历史改为FIFO队列。好处的明显的，FIFO更简易，耗用资源更少，但是相比LRU-2会降低缓存命中率。

<img src="/img/image-20220605095812371.png" alt="image-20220605095812371" style="zoom:80%;" />

```
// 直接继承LinkedHashMap
public class TwoQueueCache extends LinkedHashMap<Integer, Integer> {

    private int k; // 进入缓存队列的评判标准
    private int historyCapacity; // 访问数据历史记录最大大小
    private LRUCache lruCache; // 我们前面写好的LRUCache

    public TwoQueueCache(int cacheSize, int historyCapacity, int k) {
        // 注意这里设置LinkedHashMap的accessOrder为false
        super();
        this.historyCapacity = historyCapacity;
        this.k = k;
        this.lruCache = new LRUCache(cacheSize);
    }

    public Integer get(Integer key) {
        // 记录数据访问记录
        Integer historyCount = super.get(key);
        historyCount = historyCount == null ? 0 : historyCount;
        super.put(key, historyCount);
        return lruCache.get(key);
    }

    public Integer put(Integer key, Integer value) {

        if (value == null) {
            return null;
        }

        // 如果已经在缓存里则直接返回缓存中的数据
        if (lruCache.get(key) != null) {
            return lruCache.put(key, value);
        }

         // 如果数据历史访问次数达到上限，则加入缓存
        Integer historyCount = super.get(key);
        historyCount = historyCount == null ? 0 : historyCount;
        if (historyCount >= k) {
            // 移除历史访问记录
            super.remove(key);
            return lruCache.put(key, value);
        }

        return null;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry eldest) {
        return super.size() >= historyCapacity;
    }
}
```

这里直接继承LinkedHashMap，并且accessOrder默认为false，意为按照插入顺序进行排序，二者结合即为一个FIFO的队列。通过重写removeEldestEntry()方法来自动淘汰最早插入的数据。

### Multi Queue

相比于上面两种优化，Multi Queue的实现则复杂的多，顾名思义，Multi Queue是由多个LRU队列组成的。每一个LRU队列都有一个相应的优先级，数据会根据访问次数计算出相应的优先级，并放在该队列中。

<img src="/img/image-20220605095858203.png" alt="image-20220605095858203" style="zoom: 80%;" />

- 数据插入和访问：当数据首次插入时，会放入到优先级最低的Q0队列。当再次访问时，根据LRU的规则，会移至队列头部。当根据访问次数计算的优先级提升后，会将该数据移至更高优先级的队列的头部，并删除原队列的该数据。同样的，当该数据的优先级降低时，会移至低优先级的队列中。
- 数据淘汰：数据淘汰总是从最低优先级的队列的末尾数据进行，并将它加入到Q-history队列的头部。如果数据在Q-history数据中被访问，则重新计算该数据的优先级，并将它加入到相应优先级的队列中。否则就是按照LRU算法完全淘汰。

Multi Queue也可以看做是LRU-K的变种，将原来两个队列扩展为多个队列，好处就是无论是加入缓存还是淘汰缓存数据都变得更加细腻，但是会带来额外开销。



# 数据结构-1亿数据中访问某数据

先来看一下都有哪些题目：

- 如何从大量的 URL 中找出相同的 URL？（百度）
- 如何从大量数据中找出高频词？（百度）
- 如何找出某一天访问百度网站最多的 IP？（百度）
- 如何在大量的数据中找出不重复的整数？（百度）
- 如何在大量的数据中判断一个数是否存在？（腾讯）
- 如何查询最热门的查询串？（腾讯）
- 如何统计不同电话号码的个数？（百度）
- 如何从 5 亿个数中找出中位数？（百度）
- 如何按照 query 的频度排序？（百度）
- 如何找出排名前 500 的数？（腾讯）

答案呢？往下看~

## 如何从大量的 URL 中找出相同的 URL？

**题目描述**

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。

**解答思路**

每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。

> 5,000,000,000 * 64B ≈ 5GB * 64 = 320GB

由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用**分治策略**，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

**思路如下**：

首先遍历文件 a，对遍历到的 URL 求 `hash(URL) % 1000`，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。

接着遍历 ai( `i∈[0,999]`)，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

**方法总结**

1. 分而治之，进行哈希取余；
2. 对每个子文件进行 HashSet 统计。

## 如何从大量数据中找出高频词？

**题目描述**

有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

**解答思路**

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

**思路如下**：

首先遍历大文件，对遍历到的每个词x，执行 `hash(x) % 5000`，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 `map.put(x, 1)`；若存在，则执行 `map.put(x, map.get(x)+1)`，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个**小顶堆**来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个**小顶堆**，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为**小顶堆**，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

**方法总结**

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

##  如何找出某一天访问百度网站最多的 IP？

**题目描述**

现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

**解答思路**

这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

> 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。

**方法总结**

1. 分而治之，进行哈希取余；
2. 使用 HashMap 统计频数；
3. 求解**最大**的 TopN 个，用**小顶堆**；求解**最小**的 TopN 个，用**大顶堆**。

##  如何在大量的数据中找出不重复的整数？

**题目描述**

在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

**解答思路**

**方法一：分治法**

与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

**方法二：位图法**

**位图**，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。

位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。

假设我们要对 `[0,7]` 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：

```
0 0 0 0 0 0 0 0
复制代码
```

然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：

```
0 0 0 0 1 0 1 0
复制代码
```

依次遍历，结束后，位数组是这样的：

```
0 1 1 0 1 1 1 0
复制代码
```

每个为 1 的位，它的下标都表示了一个数：

```arduino
for i in range(8):
    if bits[i] == 1:
        print(i)
复制代码
```

这样我们其实就已经实现了排序。

对于整数相关的算法的求解，**位图法**是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。

**那么对于这道题**，我们用 2 个 bit 来表示各个数字的状态：

- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：

遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

**方法总结**

**判断数字是否重复的问题**，位图法是一种非常高效的方法。

## 如何在大量的数据中判断一个数是否存在？

**题目描述**

给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

**解答思路**

**方法一：分治法**

依然可以用分治法解决，方法与前面类似，就不再次赘述了。

**方法二：位图法**

40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。

我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

**方法总结**

**判断数字是否存在、判断数字是否重复的问题**，位图法是一种非常高效的方法。

## 如何查询最热门的查询串？

**题目描述**

搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。

假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

**解答思路**

每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

**方法一：分治法**

分治法依然是一个非常实用的方法。

划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。

方法可行，但不是最好，下面介绍其他方法。

**方法二：HashMap 法**

虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。

**思路如下**：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 `O(N)`。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 `O(Nlog10)`。

**方法三：前缀树法**

方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

**思路如下**：

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。

**方法总结**

前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

## 如何统计不同电话号码的个数？

**题目描述**

已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

**解答思路**

这道题本质还是求解**数据重复**的问题，对于这类问题，一般首先考虑位图法。

对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。

**思路如下**：

申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。

**方法总结**

求解数据重复问题，记得考虑位图法。

## 如何从 5 亿个数中找出中位数？

**题目描述**

从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 `(N+1)/2` 个数；当样本数为偶数时，中位数为 第 `N/2` 个数与第 `1+N/2` 个数的均值。

**解答思路**

如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 `O(NlogN)`。这里使用其他方法。

**方法一：双堆法**

维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数**小于等于**小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。

若数据总数为**偶数**，当这两个堆建好之后，**中位数就是这两个堆顶元素的平均值**。当数据总数为**奇数**时，根据两个堆的大小，**中位数一定在数据多的堆的堆顶**。

```ini
class MedianFinder {
    
    private PriorityQueue<Integer> maxHeap;
    private PriorityQueue<Integer> minHeap;

    /** initialize your data structure here. */
    public MedianFinder() {
        maxHeap = new PriorityQueue<>(Comparator.reverseOrder());
        minHeap = new PriorityQueue<>(Integer::compareTo);
    }
    
    public void addNum(int num) {
        if (maxHeap.isEmpty() || maxHeap.peek() > num) {
            maxHeap.offer(num);
        } else {
            minHeap.offer(num);
        }
        
        int size1 = maxHeap.size();
        int size2 = minHeap.size();
        if (size1 - size2 > 1) {
            minHeap.offer(maxHeap.poll());
        } else if (size2 - size1 > 1) {
            maxHeap.offer(minHeap.poll());
        }
    }
    
    public double findMedian() {
        int size1 = maxHeap.size();
        int size2 = minHeap.size();
        
        return size1 == size2 
            ? (maxHeap.peek() + minHeap.peek()) * 1.0 / 2
            : (size1 > size2 ? maxHeap.peek() : minHeap.peek());
    }
}
复制代码
```

> 见 LeetCode No.295：[leetcode.com/problems/fi…](https://link.juejin.cn/?target=https%3A%2F%2Fleetcode.com%2Fproblems%2Ffind-median-from-data-stream%2F)

以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法**适用于数据量较小的情况**。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。

**方法二：分治法**

分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。

对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。

划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。

> **提示**，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。

对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。

> **注意**，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

**方法总结**

分治法，真香！

## 如何按照 query 的频度排序？

**题目描述**

有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。

**解答思路**

如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。

**方法一：HashMap 法**

如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。

**方法二：分治法**

分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 `hash(query) % 10` 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。

接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。

**方法总结**

- 内存若够，直接读入进行排序；
- 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

## 如何找出排名前 500 的数？

**题目描述**

有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？

**解答思路**

对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：

首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。

接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。

重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。

> 为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。

```ini
import lombok.Data;

import java.util.Arrays;
import java.util.PriorityQueue;

/**
 * @author https://github.com/yanglbme
 */
@Data
public class DataWithSource implements Comparable<DataWithSource> {
    /**
     * 数值
     */
    private int value;

    /**
     * 记录数值来源的数组
     */
    private int source;

    /**
     * 记录数值在数组中的索引
     */
    private int index;

    public DataWithSource(int value, int source, int index) {
        this.value = value;
        this.source = source;
        this.index = index;
    }

    /**
     *
     * 由于 PriorityQueue 使用小顶堆来实现，这里通过修改
     * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆
     */
    @Override
    public int compareTo(DataWithSource o) {
        return Integer.compare(o.getValue(), this.value);
    }
}


class Test {
    public static int[] getTop(int[][] data) {
        int rowSize = data.length;
        int columnSize = data[0].length;

        // 创建一个columnSize大小的数组，存放结果
        int[] result = new int[columnSize];

        PriorityQueue<DataWithSource> maxHeap = new PriorityQueue<>();
        for (int i = 0; i < rowSize; ++i) {
            // 将每个数组的最大一个元素放入堆中
            DataWithSource d = new DataWithSource(data[i][0], i, 0);
            maxHeap.add(d);
        }

        int num = 0;
        while (num < columnSize) {
            // 删除堆顶元素
            DataWithSource d = maxHeap.poll();
            result[num++] = d.getValue();
            if (num >= columnSize) {
                break;
            }

            d.setValue(data[d.getSource()][d.getIndex() + 1]);
            d.setIndex(d.getIndex() + 1);
            maxHeap.add(d);
        }
        return result;

    }

    public static void main(String[] args) {
        int[][] data = {
                {29, 17, 14, 2, 1},
                {19, 17, 16, 15, 6},
                {30, 25, 20, 14, 5},
        };

        int[] top = getTop(data);
        System.out.println(Arrays.toString(top)); // [30, 29, 25, 20, 19]
    }
}
复制代码
```

**方法总结**

求 TopK，不妨考虑一下堆排序？
